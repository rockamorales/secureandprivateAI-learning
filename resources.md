# Matrix Calculus

https://explained.ai/matrix-calculus/index.html

# Autoaugment Github repo

https://github.com/DeepVoltaire/AutoAugment


# Types of optimization algorithms used in NN

https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f

# Inverse cooking
https://research.fb.com/publications/inverse-cooking-recipe-generation-from-food-images/

# Pytorch Internal Architecture
http://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/

# Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names
https://gombru.github.io/2018/05/23/cross_entropy_loss/

# Dropout for regularizing
https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/

# Linear Algebra

https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/


# Formal definition of differential privacy
https://medium.com/@garg4.ag/the-formal-definition-of-differential-privacy-udacityfacebookscholarship-challange-a344dbf4453

# Backpropagation
https://medium.com/@sauravkumarsct/playing-with-backpropagation-algorithm-intuition-10c42578a8e8

# Gradient descendent Backpropagation
https://medium.com/@sauravkumarsct/playing-with-gradient-descent-intuition-e5bde385078
https://medium.com/@sauravkumarsct/playing-with-backpropagation-algorithm-intuition-10c42578a8e8

# More resources
https://airtable.com/shrSyz0zaGM52kksj/tbli6XwcjDSByLwLL/viw8FIuR24ToQ47Od?blocks=hide
